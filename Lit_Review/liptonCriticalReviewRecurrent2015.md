# A Critical Review of Recurrent Neural Networks for Sequence Learning

![[liptonCriticalReviewRecurrent2015-zotero#Metadata]]

Other files:
* Mdnotes File Name: [[liptonCriticalReviewRecurrent2015]]
* Metadata File Name: [[liptonCriticalReviewRecurrent2015-zotero]]

##  Zotero links
* [Local library](zotero://select/items/1_BUVBMIFN)
* [Cloud library](http://zotero.org/users/4968335/items/BUVBMIFN)

## Notes
- 

* Mdnotes File Name: [[liptonCriticalReviewRecurrent2015]]

# Annotations  

# Annotations  
(9/23/2022, 4:19:22 PM)

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22UXPIJGPR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B148.712%2C511.84%2C477.481%2C520.687%5D%2C%5B133.768%2C499.885%2C477.48%2C508.732%5D%2C%5B133.768%2C487.93%2C477.478%2C496.777%5D%2C%5B133.768%2C475.975%2C227.309%2C484.822%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%222%22%7D%7D">“However, despite their power, standard neural networks have limitations. Most notably, they rely on the assumption of independence among the training and test examples. After each example (data point) is processed, the entire state of the network is lost.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 2</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22F758AU5P%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B133.768%2C380.333%2C477.484%2C389.18%5D%2C%5B133.768%2C368.378%2C477.476%2C377.225%5D%2C%5B133.768%2C356.423%2C258.364%2C365.27%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%222%22%7D%7D">“Recurrent neural networks (RNNs) are connectionist models with the ability to selectively pass information across sequence steps, while processing sequential data one element at a time.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 2</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22U2CBXC95%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%222%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B334.235%2C296.647%2C477.479%2C305.494%5D%2C%5B133.768%2C284.692%2C477.481%2C293.539%5D%2C%5B133.768%2C272.737%2C477.484%2C281.584%5D%2C%5B133.768%2C260.782%2C477.484%2C269.629%5D%2C%5B133.768%2C248.826%2C477.479%2C257.673%5D%2C%5B133.768%2C236.871%2C477.484%2C245.718%5D%2C%5B133.768%2C224.916%2C477.483%2C233.763%5D%2C%5B133.768%2C212.961%2C477.48%2C221.808%5D%2C%5B133.768%2C201.006%2C477.476%2C209.853%5D%2C%5B133.768%2C189.051%2C417.418%2C197.898%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%222%22%7D%7D">“To be clear, we are motivated by a desire to achieve empirical results. This motivation warrants clarification because recurrent networks have roots in both cognitive modeling and supervised machine learning. Owing to this difference of perspectives, many published papers have different aims and priorities. In many foundational papers, generally published in cognitive science and computational neuroscience journals, such as [Hopfield, 1982, Jordan, 1986, Elman, 1990], biologically plausible mechanisms are emphasized. In other papers [Schuster and Paliwal, 1997, Socher et al., 2014, Karpathy and Fei-Fei, 2014], biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 2</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22XXC5GLUE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B201.484%2C303.711%2C477.477%2C312.558%5D%2C%5B133.768%2C291.756%2C477.481%2C300.603%5D%2C%5B133.768%2C279.801%2C477.481%2C288.648%5D%2C%5B133.768%2C267.846%2C477.484%2C276.693%5D%2C%5B133.768%2C255.89%2C477.481%2C264.737%5D%2C%5B133.768%2C243.935%2C477.475%2C252.782%5D%2C%5B133.768%2C231.98%2C413.593%2C240.827%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%223%22%7D%7D">“Markov chains, which model transitions between states in an observed sequence, were first described by the mathematician Andrey Markov in 1906. Hidden Markov models (HMMs), which model an observed sequence as probabilistically dependent upon a sequence of unobserved states, were described in the 1950s and have been widely studied since the 1960s [Stratonovich, 1960]. However, traditional Markov model approaches are limited because their states must be drawn from a modestly sized discrete state space S”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%223%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 3</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22EEANHKRG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B298.438%2C405.787%2C477.479%2C414.634%5D%2C%5B133.768%2C393.831%2C477.477%2C402.678%5D%2C%5B133.768%2C381.876%2C477.479%2C390.723%5D%2C%5B133.768%2C369.921%2C477.478%2C378.768%5D%2C%5B133.768%2C357.966%2C477.477%2C366.813%5D%2C%5B133.768%2C346.011%2C219.254%2C354.858%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%224%22%7D%7D">“A well-known result is that a finite-sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine [Siegelmann and Sontag, 1991]. The capability of RNNs to perform arbitrary computation demonstrates their expressive power, but one could argue that the C programming language is equally capable of expressing arbitrary programs.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%224%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 4</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22VWENPZXE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B148.712%2C583.571%2C476.981%2C593.96%5D%2C%5B133.768%2C571.616%2C477.476%2C580.463%5D%2C%5B133.768%2C559.661%2C252.188%2C570.049%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%7D">“Using temporal terminology, an input sequence consists of data points x(t) that arrive in a discrete sequence of time steps indexed by t. A target sequence consists of data points y(t).”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 6</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22HXZNWH3X%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B148.712%2C511.84%2C477.482%2C520.687%5D%2C%5B133.768%2C499.885%2C252.573%2C508.732%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%7D">“The time-indexed data points may be equally spaced samples from a continuous real-world process.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 6</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22JGJMCQXU%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B255.481%2C475.975%2C477.483%2C484.822%5D%2C%5B133.768%2C464.02%2C477.483%2C472.867%5D%2C%5B133.768%2C452.064%2C440.297%2C460.911%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%7D">“The time steps may also be ordinal, with no exact correspondence to durations. In fact, RNNs are frequently applied to domains where sequences have a defined order but no explicit notion of time.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 6</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22U4LNCK4B%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B383.47%2C346.476%2C477.476%2C355.323%5D%2C%5B133.768%2C333.607%2C477.475%2C343.926%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%7D">“Associated with each neuron j is an activation function lj(·), which is sometimes called a link function.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 6</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22TIZL5WBF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B148.712%2C285.786%2C428.42%2C297.479%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%7D">“Associated with each edge from node j′ to j is a weight wjj′ .”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 6</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22JVHZ6NJJ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%226%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B148.712%2C190.145%2C477.479%2C199.906%5D%2C%5B133.768%2C179.104%2C351.509%2C187.951%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%7D">“The value vj of each neuron j is calculated by applying its activation function to a weighted sum of the values of its input nodes”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%226%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 6</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22AQJN882K%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B133.768%2C445.925%2C272.698%2C455.686%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%227%22%7D%7D">“activation and notate it as aj.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 7</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22T4R3CAAH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B148.712%2C279.466%2C477.478%2C288.313%5D%2C%5B133.768%2C267.511%2C477.482%2C276.358%5D%2C%5B133.768%2C255.556%2C267.284%2C264.403%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%227%22%7D%7D">“The activation function at the output nodes depends upon the task. For multiclass classification with K alternative classes, we apply a softmax nonlinearity in an output layer of K nodes.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 7</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%224NB5T6UY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B357.644%2C187.802%2C477.48%2C196.649%5D%2C%5B133.768%2C175.847%2C477.475%2C184.694%5D%2C%5B133.768%2C163.892%2C257.219%2C172.739%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%227%22%7D%7D">“For multilabel classification the activation function is simply a point-wise sigmoid, and for regression we typically have linear output.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%227%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 7</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22KIYHJRMG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B242.095%2C342.9%2C477.476%2C351.747%5D%2C%5B133.768%2C330.945%2C477.484%2C339.792%5D%2C%5B133.768%2C318.99%2C477.476%2C327.837%5D%2C%5B133.768%2C307.034%2C477.483%2C315.881%5D%2C%5B133.768%2C295.079%2C204.452%2C303.926%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%228%22%7D%7D">“Feedforward networks (Figure 2) are a restricted class of networks which deal with this problem by forbidding cycles in the directed graph of nodes. Given the absence of cycles, all nodes can be arranged into layers, and the outputs in each layer can be calculated given the outputs from the lower layers.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 8</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22DKJCC5XN%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B428.654%2C247.258%2C477.484%2C256.105%5D%2C%5B133.768%2C235.303%2C477.48%2C244.15%5D%2C%5B133.768%2C223.348%2C169.469%2C232.753%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%228%22%7D%7D">“Learning is accomplished by iteratively updating each of the weights to minimize a loss function”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 8</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22KT5JBWJT%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B403.975%2C187.483%2C477.476%2C196.33%5D%2C%5B133.768%2C175.527%2C477.476%2C184.932%5D%2C%5B133.768%2C163.572%2C477.478%2C172.419%5D%2C%5B133.768%2C151.617%2C185.478%2C160.464%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%228%22%7D%7D">“Backpropagation uses the chain rule to calculate the derivative of the loss function L with respect to each parameter in the network. The weights are then adjusted by gradient descent.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%228%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 8</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22AWZPV3RV%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B148.712%2C595.526%2C477.484%2C604.373%5D%2C%5B133.768%2C583.571%2C276.512%2C592.418%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%7D">“Nowadays, neural networks are usually trained with stochastic gradient descent (SGD) using mini-batches.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 9</span>)</span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 9</span>)</span> Adam optimizer is another popular SGD solver

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22PK34WUYL%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B133.768%2C470.786%2C477.477%2C479.633%5D%2C%5B133.768%2C458.831%2C477.48%2C467.678%5D%2C%5B133.768%2C446.875%2C477.484%2C455.722%5D%2C%5B133.768%2C434.92%2C210.654%2C443.767%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%7D">“AdaGrad, arguably the most popular, adapts the learning rate by caching the sum of squared gradients with respect to each parameter at each time step. The step size for each feature is multiplied by the inverse of the square root of this cached value.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 9</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22774HIUG2%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B225.393%2C399.055%2C477.479%2C407.902%5D%2C%5B133.768%2C387.1%2C477.484%2C395.947%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%7D">“RMSprop modifies AdaGrad by introducing a decay factor in the cache, changing the monotonically growing value into a moving average.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 9</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22EY6WPYX3%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%229%22%2C%22position%22%3A%7B%22pageIndex%22%3A8%2C%22rects%22%3A%5B%5B133.768%2C375.144%2C477.477%2C383.991%5D%2C%5B133.768%2C363.189%2C477.481%2C372.036%5D%2C%5B133.768%2C351.234%2C170.633%2C360.081%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%7D">“Momentum methods are another common SGD variant used to train neural networks. These methods add to each update a decaying sum of the previous updates.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 9</span>)</span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%229%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 9</span>)</span> Backpropagation for a feedforward NN:  
1. Pass an example through the network to obtain a y_hat. Calculate the loss function L.  
2. For each output node obtain the derivative of the loss wrt the predicted value times the derivative of the activation function evaluated at the activation level.  
3. For each gradient from the output nodes, calculate the gradient of each of the connected outgoing nodes in the previous layer. This is the derivative of the activation function times the output derivatives weighted by the initial weights.  
  
4. Calculate the derivative of the loss wrt each weight which is the derivative of the "to" node times the value of the "from" node.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22N366PN45%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B148.712%2C459.053%2C477.476%2C467.9%5D%2C%5B133.768%2C447.098%2C477.477%2C455.945%5D%2C%5B133.768%2C435.143%2C477.483%2C443.99%5D%2C%5B133.768%2C423.188%2C477.475%2C432.035%5D%2C%5B133.768%2C411.233%2C440.921%2C420.08%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2210%22%7D%7D">“One open question in neural network research is how to exploit sparsity in training. In a neural network with sigmoidal or tanh activation functions, the nodes in each layer never take value exactly zero. Thus, even if the inputs are sparse, the nodes at each hidden layer are not. However, rectified linear units (ReLUs) introduce sparsity to hidden layers [Glorot et al., 2011]”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 10</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22B7MDUGY9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2210%22%2C%22position%22%3A%7B%22pageIndex%22%3A9%2C%22rects%22%3A%5B%5B133.768%2C283.589%2C477.478%2C292.436%5D%2C%5B133.768%2C271.633%2C477.475%2C280.48%5D%2C%5B133.768%2C259.678%2C193.879%2C268.525%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2210%22%7D%7D">“Recurrent neural networks are feedforward neural networks augmented by the inclusion of edges that span adjacent time steps, introducing a notion of time to the model.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2210%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 10</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22ZBKZ6UF6%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2211%22%2C%22position%22%3A%7B%22pageIndex%22%3A10%2C%22rects%22%3A%5B%5B337.418%2C330.332%2C477.479%2C339.179%5D%2C%5B133.768%2C318.377%2C477.477%2C327.224%5D%2C%5B133.768%2C306.422%2C477.475%2C315.269%5D%2C%5B133.768%2C294.467%2C477.475%2C303.314%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2211%22%7D%7D">“Given this picture, the network can be interpreted not as cyclic, but rather as a deep network with one layer per time step and shared weights across time steps. It is then clear that the unfolded network can be trained across many time steps using backpropagation.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2211%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 11</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22P87QKLTH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2213%22%2C%22position%22%3A%7B%22pageIndex%22%3A12%2C%22rects%22%3A%5B%5B460.318%2C243.28%2C477.476%2C252.127%5D%2C%5B133.768%2C231.325%2C477.479%2C240.172%5D%2C%5B133.768%2C219.37%2C255.434%2C228.217%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2213%22%7D%7D">“The problems of vanishing and exploding gradients occur when backpropagating errors across many time steps.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2213%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 13</span>)</span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2213%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 13</span>)</span> Vanishing gradients are taken care of by GRU.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22L6X69NN5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2214%22%2C%22position%22%3A%7B%22pageIndex%22%3A13%2C%22rects%22%3A%5B%5B148.712%2C409.11%2C477.479%2C417.957%5D%2C%5B133.768%2C396.24%2C477.483%2C406.56%5D%2C%5B133.768%2C385.2%2C477.481%2C394.047%5D%2C%5B133.768%2C373.244%2C477.482%2C382.091%5D%2C%5B133.768%2C361.289%2C311.798%2C370.136%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2214%22%7D%7D">“Which of the two phenomena occurs depends on whether the weight of the recurrent edge |wjj| &gt; 1 or |wjj| &lt; 1 and on the activation function in the hidden node (Figure 8). Given a sigmoid activation function, the vanishing gradient problem is more pressing, but with a rectified linear unit max(0, x), it is easier to imagine the exploding gradient.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2214%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 14</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%227VESN6UJ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2214%22%2C%22position%22%3A%7B%22pageIndex%22%3A13%2C%22rects%22%3A%5B%5B148.712%2C301.513%2C477.484%2C310.36%5D%2C%5B133.768%2C289.558%2C477.482%2C298.405%5D%2C%5B133.768%2C277.603%2C191.814%2C286.45%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2214%22%7D%7D">“Truncated backpropagation through time (TBPTT) is one solution to the exploding gradient problem for continuously running networks [Williams and Zipser, 1989].”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2214%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 14</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%224MYTXN57%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2214%22%2C%22position%22%3A%7B%22pageIndex%22%3A13%2C%22rects%22%3A%5B%5B274.098%2C265.648%2C477.484%2C274.495%5D%2C%5B133.768%2C253.693%2C477.478%2C262.54%5D%2C%5B133.768%2C241.738%2C477.477%2C250.585%5D%2C%5B133.768%2C229.782%2C477.479%2C238.629%5D%2C%5B133.768%2C217.827%2C342.655%2C226.674%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2214%22%7D%7D">“While TBPTT with a small cutoff can be used to alleviate the exploding gradient problem, it requires that one sacrifice the ability to learn long-range dependencies. The LSTM architecture described below uses carefully designed nodes with recurrent edges with fixed unit weight as a solution to the vanishing gradient problem.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2214%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 14</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22QQ5M8EWE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2216%22%2C%22position%22%3A%7B%22pageIndex%22%3A15%2C%22rects%22%3A%5B%5B239.11%2C288.362%2C477.48%2C297.209%5D%2C%5B133.768%2C276.406%2C477.48%2C285.253%5D%2C%5B133.768%2C264.451%2C421.645%2C273.298%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2216%22%7D%7D">“The first paper, Long Short-Term Memory by Hochreiter and Schmidhuber [1997], introduces the memory cell, a unit of computation that replaces traditional nodes in the hidden layer of a network.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2216%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 16</span>)</span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2216%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 16</span>)</span> This review is from before GRU.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22JXDVMJXM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2216%22%2C%22position%22%3A%7B%22pageIndex%22%3A15%2C%22rects%22%3A%5B%5B292.825%2C240.541%2C477.482%2C249.388%5D%2C%5B133.768%2C228.586%2C477.479%2C237.433%5D%2C%5B133.768%2C216.631%2C477.481%2C225.478%5D%2C%5B133.768%2C204.675%2C294.553%2C213.522%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2216%22%7D%7D">“The second paper, Bidirectional Recurrent Neural Networks by Schuster and Paliwal [1997], introduces an architecture in which information from both the future and the past are used to determine the output at any point in the sequence.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2216%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 16</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22PSXPEX9A%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2217%22%2C%22position%22%3A%7B%22pageIndex%22%3A16%2C%22rects%22%3A%5B%5B344.552%2C329.07%2C477.477%2C337.917%5D%2C%5B133.768%2C317.115%2C477.483%2C325.962%5D%2C%5B133.768%2C305.16%2C477.48%2C314.007%5D%2C%5B133.768%2C293.205%2C477.483%2C302.052%5D%2C%5B133.768%2C281.25%2C477.477%2C290.097%5D%2C%5B133.768%2C269.294%2C477.48%2C278.141%5D%2C%5B133.768%2C257.339%2C235.284%2C266.186%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2217%22%7D%7D">“the LSTM model primarily in order to overcome the problem of vanishing gradients. This model resembles a standard recurrent neural network with a hidden layer, but each ordinary node (Figure 1) in the hidden layer is replaced by a memory cell (Figure 9). Each memory cell contains a node with a self-connected recurrent edge of fixed weight one, ensuring that the gradient can pass across many time steps without vanishing or exploding.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2217%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 17</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22G3FR86YI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2217%22%2C%22position%22%3A%7B%22pageIndex%22%3A16%2C%22rects%22%3A%5B%5B397.551%2C185.608%2C477.479%2C194.455%5D%2C%5B133.768%2C173.653%2C477.484%2C182.5%5D%2C%5B133.768%2C161.698%2C477.481%2C170.545%5D%2C%5B133.768%2C149.743%2C340.684%2C158.59%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2217%22%7D%7D">“The LSTM model introduces an intermediate type of storage via the memory cell. A memory cell is a composite unit, built from simpler nodes in a specific connectivity pattern, with the novel inclusion of multiplicative nodes,”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2217%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 17</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22X7NISD7E%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2218%22%2C%22position%22%3A%7B%22pageIndex%22%3A17%2C%22rects%22%3A%5B%5B158.675%2C610.261%2C477.48%2C620.581%5D%2C%5B158.675%2C599.22%2C477.481%2C609.609%5D%2C%5B158.675%2C586.048%2C477.48%2C597.666%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%7D">“Input node: This unit, labeled gc, is a node that takes activation in the standard way from the input layer x(t) at the current time step and (along recurrent edges) from the hidden layer at the previous time step h(t−1).”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 18</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22EUMQVE6H%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2218%22%2C%22position%22%3A%7B%22pageIndex%22%3A17%2C%22rects%22%3A%5B%5B158.675%2C530.937%2C477.478%2C540.342%5D%2C%5B158.675%2C518.982%2C477.476%2C527.829%5D%2C%5B158.675%2C507.027%2C477.477%2C517.415%5D%2C%5B158.675%2C495.072%2C202.784%2C503.919%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%7D">“Input gate: Gates are a distinctive feature of the LSTM approach. A gate is a sigmoidal unit that, like the input node, takes activation from the current data point x(t) as well as from the hidden layer at the previous time step.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 18</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22CX5LYLRQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2218%22%2C%22position%22%3A%7B%22pageIndex%22%3A17%2C%22rects%22%3A%5B%5B264.018%2C483.117%2C477.484%2C491.964%5D%2C%5B158.675%2C471.161%2C331.13%2C480.008%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%7D">“It is a gate in the sense that if its value is zero, then flow from the other node is cut off.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 18</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%227X9DQFB5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2218%22%2C%22position%22%3A%7B%22pageIndex%22%3A17%2C%22rects%22%3A%5B%5B158.675%2C427.092%2C477.476%2C437.411%5D%2C%5B158.675%2C416.051%2C477.475%2C424.898%5D%2C%5B158.675%2C403.182%2C477.478%2C412.943%5D%2C%5B158.675%2C392.141%2C257.748%2C400.988%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%7D">“Internal state: At the heart of each memory cell is a node sc with linear activation, which is referred to in the original paper as the “internal state” of the cell. The internal state sc has a self-connected recurrent edge with fixed unit weight.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 18</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22JKJQB3GC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2218%22%2C%22position%22%3A%7B%22pageIndex%22%3A17%2C%22rects%22%3A%5B%5B158.675%2C324.161%2C477.482%2C334.48%5D%2C%5B158.675%2C313.12%2C477.477%2C321.967%5D%2C%5B158.675%2C301.165%2C251.782%2C310.012%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%7D">“Forget gate: These gates fc were introduced by Gers et al. [2000]. They provide a method by which the network can learn to flush the contents of the internal state.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 18</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%22B95Y9M22%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2218%22%2C%22position%22%3A%7B%22pageIndex%22%3A17%2C%22rects%22%3A%5B%5B158.675%2C232.263%2C477.478%2C242.582%5D%2C%5B158.675%2C220.308%2C477.481%2C230.069%5D%2C%5B158.675%2C208.352%2C170.331%2C218.114%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%7D">“Output gate: The value vc ultimately produced by a memory cell is the value of the internal state sc multiplied by the value of the output gate oc.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2218%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 18</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FSVNC9UMJ%22%2C%22annotationKey%22%3A%224J5EPF88%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%2220%22%2C%22position%22%3A%7B%22pageIndex%22%3A19%2C%22rects%22%3A%5B%5B148.712%2C141.328%2C477.484%2C150.175%5D%2C%5B133.768%2C129.373%2C477.477%2C138.22%5D%2C%5B133.768%2C117.418%2C245.611%2C126.265%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2220%22%7D%7D">“Intuitively, in terms of the forward pass, the LSTM can learn when to let activation into the internal state. As long as the input gate takes value zero, no activation can get in.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FBUVBMIFN%22%5D%2C%22locator%22%3A%2220%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Lipton et al., 2015, p. 20</span>)</span>

