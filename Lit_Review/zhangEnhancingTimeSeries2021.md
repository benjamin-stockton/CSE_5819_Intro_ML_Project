# Enhancing Time Series Predictors with Generalized Extreme Value Loss

![[zhangEnhancingTimeSeries2021-zotero#Metadata]]

Other files:
* Mdnotes File Name: [[zhangEnhancingTimeSeries2021]]
* Metadata File Name: [[zhangEnhancingTimeSeries2021-zotero]]

##  Zotero links
* [Local library](zotero://select/items/1_D66KN9JD)
* [Cloud library](http://zotero.org/users/4968335/items/D66KN9JD)

## Notes
- 

* Mdnotes File Name: [[zhangEnhancingTimeSeries2021]]

# Annotations(2022-09-29)
“As summarized in [8], an extreme event in time series is featured with an extremely small or large value, a relatively low frequency in time series data and is usually out of the observed dynamics underlying the time series.” (Zhang et al., 2021, p. 1)“Nowadays, with increasingly more applications of time series prediction to safety-critical tasks (e.g., weather forecasting [9] and market monitoring [10]), a more accurate modeling of extreme events in time series may aid administration decisions to preserve and enhance social goods if more in-time alarms on future incidents are provided to the people.” (Zhang et al., 2021, p. 1)“Due to its relatively low proportion in time series data, prediction errors on extreme events would only slightly affect the average prediction accuracy (Fig. 1), which may serve a major reason of why existing methods rarely explicitly model the extreme events in their proposed solutions.” (Zhang et al., 2021, p. 1)“Specifically, we leverage the Bayesian theory to derive the behavioral equivalence of the prediction of a DNN trained with the square loss with that of a Gaussian kernel density estimator (KDE) over the observed event values. As shown in Fig. 2(b), due to the rarity nature of extreme events, the distribution of event values has a heavy tail part where lies the extreme events, while the Gaussian KDE is by definition light-tailed.” (Zhang et al., 2021, p. 2)“Instead of presenting a novel time series predictor to outperform the state-of-the-art time series predictors in modeling extreme events, we propose a general loss form called Generalized Extreme Value Loss (GEVL), which incorporates the heavy-tailed nature of time series data into the classical square loss by different choices of kernel functions.” (Zhang et al., 2021, p. 2)“we propose to dynamically shift the heavy tail of the ground-truth data distributions to convert the original distribution to a light-tailed one, which equivalently results in a GEVL based on a shifted Gaussian kernel” (Zhang et al., 2021, p. 2)“we explicitly design two heavy-tailed kernel functions, namely, Gumbel kernel and Fr ́ echet kernel, and use the kernels to derive heavy-tailed KDEs and the corresponding GEVLs.” (Zhang et al., 2021, p. 2)“We propose Generalized Extreme Value Loss (GEVL), a generic loss form which allows the derivation of different loss functions from heavy-tailed distributions.” (Zhang et al., 2021, p. 2)“In practice, we reinterpret our earlier work as the GEVL derived from a shifted Gaussian kernel and present two alternative loss forms for time series prediction.” (Zhang et al., 2021, p. 2)“We unify our earlier work into our novel framework of GEVL in Sections 4.1 & 4.2, and propose two alternative implementations of GEVL, namely, with Gumbel and Fr ́ echet kernels respectively in Section 4.3.” (Zhang et al., 2021, p. 2)“In this paper, we mainly focus on the prediction task of many-to-one univariate time series, a common setting in a wide range of practical scenarios including finance, meteorology and many more [1].” (Zhang et al., 2021, p. 2)“To present a formal definition of extreme events in time series, we first define an auxiliary sequence of extreme event indicators V1:T = [v1, · · · , vT ]” (Zhang et al., 2021, p. 3)“Definition 1 (Types of Events). For a given t-th time step, an event yt is called a normal event, a right extreme event or a left extreme events respectively for vt = 0, vt = 1 or vt = −1.” (Zhang et al., 2021, p. 3)“, our work assumes the extreme events are real events in a recorded time series instead of noisy inputs.” (Zhang et al., 2021, p. 3)“For example, a majority of commonly used distributions including Gaussian, Poisson and Gamma distributions are light-tailed, while, to name a few, Pareto, Weibuil, Fr ́ echet distributions are heavy-tailed.” (Zhang et al., 2021, p. 3)“the tail property of a KDE is largely determined by its underlying kernel,” (Zhang et al., 2021, p. 4)“From the probabilistic perspective, minimizing the square loss ‖yt − ot‖ in Objective 1 is equivalent to maximizing the likelihood of a Gaussian distribution” (Zhang et al., 2021, p. 4)“Under the mild assumption that the DNN-based model fθ has a sufficiently large learning capacity [31], [32], maximizing the likelihood above would yield an optimal estimator of pY |X [33]” (Zhang et al., 2021, p. 4)“Finally, considering the fact that a model optimized by maximizing the likelihood is a discriminative model (Eq. 1), i.e., fθ has no prior on yt, we reach the intermediate conclusion that the distribution pY has the same form as a Gaussian KDE over y1, y2, . . . , yn,” (Zhang et al., 2021, p. 4)“once a model achieves the minimal square loss (which can be easily satisfied with a DNN-based model), the marginal probability density of its prediction, i.e., pY , is always identical to a Gaussian KDE” (Zhang et al., 2021, p. 4)“s the ground-truth event distributions in real world are usually heavy-tailed [8], the misalignment between the tail parts of the light-tailed Gaussian KDE and the heavy-tailed ground-truth distribution may cause the underestimation of extreme events which usually distribute at the tail.” (Zhang et al., 2021, p. 4)“In other words, a DNN-based model trained with the square loss tend to be conservative in predicting extreme values,” (Zhang et al., 2021, p. 5)“Transformation on Data. In the first approach, we are required to find a mapping to transform the heavy-tailed data distribution to a light-tailed one. After the data transformation, Gaussian KDE can be properly applied to the transformed observations, which may to some extent restore the capability of the conventional square loss” (Zhang et al., 2021, p. 5)“Transformation on Estimator. In the second approach, we are required to design novel kernel functions such that the corresponding KDE is suitable for heavy-tailed data distribution.” (Zhang et al., 2021, p. 5)“Especially when the kernel K is heavy-tailed, we call the loss form in Eq. 15 a Generalized Extreme Value Loss (GEVL).” (Zhang et al., 2021, p. 5)“The motivation behind Eq. 16 is straightforward: it left (right) shifts all the right (left) extreme events by a shared bias b multiplied with a dynamic scale |ut|.” (Zhang et al., 2021, p. 5)“Inspired from this, we propose to memorize these extreme events by a memory network (i.e., MemNN), which is proved to be effective in modeling historical information [35]” (Zhang et al., 2021, p. 7)“To achieve this, we construct the MemNN with two modules: the embedding module and the history module.” (Zhang et al., 2021, p. 7)“To better represent the information contained in each window, we propose to use a RNN to embed each window into a feature space. Specifically, we use Dj as the input, GRU as the implementation of RNN, and regard the last hidden state as the latent representation of this window,” (Zhang et al., 2021, p. 7)“Attention Mechanism. Next, we introduce how to calculate the similarity between the past windows Dj and the current time series X1:t. First, we use a GRU to embed X1:t to a hidden state ht,” (Zhang et al., 2021, p. 7)“Then we derive the similarity between time series via the distance between the embeddings ht and sj” (Zhang et al., 2021, p. 7)“The attention score αtj describes the similarity between the current time series X1:t as the query and the j-th window in the MemNN.” (Zhang et al., 2021, p. 7)“As is shown in Fig. 3(c), compared with our previous approach via data transformation, our newly proposed approach is more flexible and lightweight as it directly replaces the conventional square loss with GEVLs to supervise the base time series predictors, requiring no additional neural network modules (e.g., MemNN).” (Zhang et al., 2021, p. 7)“It is worth to note, this kernel function is an original design of our work, for which our motivation is to smooth the Gaussian kernel by the regularization term (1 − e−u2 )γ.” (Zhang et al., 2021, p. 7)“Different from the Gumbel GEVL, the Fr ́ echet GEVL is welldefined only for a nonnegative δt, the case for the rightextreme events.” (Zhang et al., 2021, p. 8)“Noteworthily, to reflect the diversity of the extreme event phenomenon in the real world [8], we carefully set the thresholds 1 and 2 to control the ratio of extreme events in Synthetic, Climate and Stock to be 5%, 7.5% and 10%” (Zhang et al., 2021, p. 8)“Metrics. To measure the quality of predicted values ot, we calculate the root mean square error (RMSE) on the test slice” (Zhang et al., 2021, p. 9)“Additionally, we use the F1 score to validate the effectiveness of our proposed estimator ut for the extreme event indicator in Section 4.2. Specifically, we first calculate the prediction ut ∈ [−1, 1]. Then we determine the predicted label by setting a threshold, i.e., ̃ ut = −1 if ut < −0.3 and ̃ ut = 1 if ut > 0.3.” (Zhang et al., 2021, p. 9)“As one of the most effective statistical methods, ARIMA forecasts the future event values based on a predefined forecasting equation, which does not involve a training process.” (Zhang et al., 2021, p. 9) It has a training process? I'm not sure what is meant here.



