# Modeling Extreme Events in Time Series Prediction

![[dingModelingExtremeEvents2019-zotero#Metadata]]

Other files:
* Mdnotes File Name: [[dingModelingExtremeEvents2019]]
* Metadata File Name: [[dingModelingExtremeEvents2019-zotero]]

##  Zotero links
* [Local library](zotero://select/items/1_2VBD3PYT)
* [Cloud library](http://zotero.org/users/4968335/items/2VBD3PYT)

## Notes
- 

* Mdnotes File Name: [[dingModelingExtremeEvents2019]]

# Annotations  

# Annotations  
(9/14/2022, 12:34:00 PM)

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%225LVNT2IZ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221114%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B470.132%2C479.697%2C558.201%2C483.786%5D%2C%5B317.955%2C468.738%2C559.702%2C472.827%5D%2C%5B317.955%2C457.779%2C558.193%2C461.868%5D%2C%5B317.955%2C446.82%2C379.894%2C450.909%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%7D">“a number of DNN based techniques have been subsequently developed for time-series prediction tasks, achieving noticeable improvements over traditional methods [11, 49].”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1114</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22MV4FEFWV%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221114%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B522.686%2C446.82%2C558.193%2C450.909%5D%2C%5B317.955%2C435.861%2C558.358%2C439.95%5D%2C%5B317.955%2C424.902%2C559.715%2C428.991%5D%2C%5B317.955%2C413.943%2C558.19%2C418.032%5D%2C%5B317.955%2C402.985%2C548.316%2C407.074%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%7D">“Recurrent Neural Network (RNN) module serves as an indispensable factor for these note-worthy improvements [31, 48]. Compared with traditional methods, one of the major advantages of RNN structure is that it enables deep non-linear modeling of temporal patterns.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1114</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22WLMWXMWF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221114%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B442.688%2C216.683%2C558.193%2C220.772%5D%2C%5B317.955%2C205.724%2C559.176%2C209.813%5D%2C%5B317.955%2C194.765%2C454.822%2C198.854%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%7D">“Intuitively, an extreme event in time series is usually featured by extremely small or large values, of irregular and rare occurrences [24].”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1114</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22LUPHLFCW%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221114%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B327.223%2C126.959%2C558.188%2C135.432%5D%2C%5B317.955%2C118.053%2C558.192%2C122.142%5D%2C%5B317.955%2C105.014%2C558.195%2C113.514%5D%2C%5B317.955%2C96.135%2C558.426%2C100.224%5D%2C%5B317.955%2C83.096%2C515.688%2C91.434%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%7D">“In Fig. 1(a), most of its predictions are bounded by thresholds and therefore it fails to recognize future extreme events, we claim this as underfitting phenomenon. b. In Fig. 1(b), although the model learns extreme events in the train set correctly, it behaves poorly on test sets, we cliam this as overfitting phenomenon.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221114%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1114</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22CN8NZAFA%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221115%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B184.869%2C394.207%2C294.036%2C398.296%5D%2C%5B53.798%2C383.248%2C294.036%2C387.337%5D%2C%5B53.798%2C372.289%2C294.034%2C376.378%5D%2C%5B53.798%2C361.33%2C271.223%2C365.419%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%7D">“With more accurate modeling of extreme events in many real-world cases, prediction models are expected to aid influential decisions by providing alarms on future incidents such as extreme winds [35] or financial crisis [41]”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22BU5IDBWK%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221115%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B133.717%2C295.577%2C294.263%2C299.666%5D%2C%5B53.529%2C284.618%2C294.035%2C288.707%5D%2C%5B53.798%2C273.659%2C294.033%2C277.748%5D%2C%5B53.798%2C262.7%2C294.032%2C266.789%5D%2C%5B53.798%2C251.741%2C294.037%2C255.83%5D%2C%5B53.798%2C240.782%2C166.508%2C244.871%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%7D">“Through the lens of Extreme Value Theory (EVT), we observe that the main reason lies in previous choices of loss function, which inherently lacks the ability to model extreme events in a find-grained way. Therefore we propose a novel form of loss called Extreme Value Loss (EVL) to improve predictions on occurrences of extreme events.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22D2N5SQHY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221115%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B62.24%2C138.084%2C294.033%2C145.75%5D%2C%5B62.24%2C128.47%2C294.031%2C132.559%5D%2C%5B62.24%2C117.511%2C175.436%2C121.6%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%7D">“We provide a formal analysis on why deep neural network suffers underfitting or overfitting phenomenons during predicting time series data with extreme value.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%225JTMYV4Z%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221115%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B62.24%2C105.207%2C294.569%2C112.873%5D%2C%5B62.24%2C95.593%2C294.033%2C99.682%5D%2C%5B62.24%2C84.635%2C211.389%2C88.724%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%7D">“We propose a novel loss function called Extreme Value Loss (EVL) based on extreme value theory, which provides better predictions on future occurrences of extreme events.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22CTE3FYCZ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221115%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B326.397%2C696.988%2C559.71%2C704.654%5D%2C%5B326.397%2C687.374%2C558.19%2C691.463%5D%2C%5B326.397%2C676.415%2C559.71%2C680.504%5D%2C%5B326.397%2C665.456%2C558.193%2C669.545%5D%2C%5B326.397%2C654.498%2C401.362%2C658.587%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%7D">“We propose a brand-new Memory Network based neural architecture to memorize extreme events in history for better predictions of future extreme values. Experimental results validates the superiority of our framework in prediction accuracy compared with the state-of-the-arts.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span> Data are a sequence of N time series of length T. The input at time t is the current observation x_t and the output is the next observation y_{t+1}.

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span> Stat methods: ARMA and NARX which are linear and non-linear regressions with exogenous variables.  
ML methods: Recurrent neural network extended to long-short term memory (LSTM) and Gated recurrent unit (GRU).

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22WPJW66V4%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221115%22%2C%22position%22%3A%7B%22pageIndex%22%3A1%2C%22rects%22%3A%5B%5B317.641%2C239.742%2C558.194%2C243.831%5D%2C%5B317.955%2C228.783%2C558.358%2C232.872%5D%2C%5B317.955%2C217.824%2C558.192%2C221.913%5D%2C%5B317.623%2C206.865%2C472.804%2C210.954%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%7D">“Although DNN such as GRU has achieved noticeable improvements in predicting time-series data, this model tends to fall into either overfitting or underfitting if trained with imbalanced time series, as we have demonstrated in introductory part”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221115%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1115</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%2294JBFRQG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221116%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B165.171%2C405.643%2C294.045%2C414.608%5D%2C%5B53.798%2C394.827%2C295.552%2C403.165%5D%2C%5B53.798%2C385.949%2C294.571%2C390.038%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%7D">“Intuitively, if a random variable Y is said to respect a heavy-tailed distribution, then it usually has a nonnegligible probability of taking large values (larger than a threshold)”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1116</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%2274IDXHTZ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221116%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B278.187%2C257.215%2C294.3%2C261.304%5D%2C%5B53.798%2C246.256%2C276.434%2C250.345%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%7D">“EVT studies the distribution of maximum in observed samples [43]”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1116</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22EJY4TYYU%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221116%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B160.926%2C147.885%2C294.031%2C151.974%5D%2C%5B53.798%2C134.702%2C294.278%2C143.668%5D%2C%5B53.798%2C125.967%2C210.927%2C130.056%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%7D">“As a fundamental result in EVT, the following theorem states that the distribution of Y after linearly transformed is always limited to few cases.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1116</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%225PPR28M3%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221116%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B492.906%2C635.654%2C558.195%2C644.62%5D%2C%5B317.955%2C626.919%2C558.194%2C631.008%5D%2C%5B317.955%2C615.96%2C371.685%2C620.049%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%7D">“Such a statement sometimes is also regarded as the law of large numbers for the maximum [27]”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1116</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22PWZQ6GPP%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221116%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B504.964%2C303.755%2C559.715%2C313.642%5D%2C%5B317.955%2C295.941%2C558.193%2C300.03%5D%2C%5B317.955%2C281.838%2C559.188%2C291.724%5D%2C%5B317.623%2C270.638%2C558.201%2C280.766%5D%2C%5B317.955%2C263.065%2C376.297%2C267.154%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%7D">“Based on Bregman’s theory [5, 40], minimizing such square loss always has the form of Gaussian with variance τ , that is, p(yt |xt , θ ) = N (ot , τ 2), where θ is the parameter of the predicting model, O1:T are outputs from the model.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1116</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22GQ65HWBG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221116%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B454.672%2C126.246%2C558.369%2C135.213%5D%2C%5B317.955%2C117.511%2C558.189%2C121.6%5D%2C%5B317.955%2C106.552%2C558.197%2C110.641%5D%2C%5B317.955%2C93.37%2C455.592%2C102.335%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%7D">“It is worth to notice that our assumption on learning capacity is a widely adopted assumption in previous researches [3, 21] and can be implemented with a deep neural network structure in practice.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221116%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1116</span>)</span> It is not at all clear what this means. I assumed it was having enough data to estimate each parameter, but implementing a DNN doesn't change the amount of data available.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22XQ5CUI3M%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221117%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B105.898%2C500.58%2C294.045%2C509.546%5D%2C%5B53.798%2C491.845%2C237.691%2C495.934%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%7D">“reasonable to state that empirical distribution P(Y ) after optimization should be of the following form”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1117</span>)</span> Sure, but we aren't interested in the marginal distribution, we care about P(Y | X). Not sure what Bayes is doing here.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22KITNE8C5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221117%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B63.761%2C323.916%2C294.039%2C328.005%5D%2C%5B53.467%2C312.957%2C294.039%2C317.046%5D%2C%5B53.798%2C301.998%2C294.036%2C306.087%5D%2C%5B53.467%2C291.039%2C156.354%2C295.128%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%7D">“Since nonparametric kernel density estimator only works well with sufficient samples, the performance therefore is expected to decrease at the tail part of the data, where sampled data points would be rather limited [7].”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1117</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22AV2D9HVQ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221117%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B327.918%2C536.863%2C558.426%2C540.952%5D%2C%5B317.955%2C525.904%2C558.192%2C529.993%5D%2C%5B317.417%2C511.8%2C328.047%2C521.688%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%7D">“As we can see, the extreme events problem in DNN is mainly caused by that there is no sufficient prior on tail part of observations yt .”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1117</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%223HZYIC72%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221117%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B507.428%2C266.366%2C558.199%2C270.455%5D%2C%5B317.955%2C253.328%2C558.2%2C261.666%5D%2C%5B317.955%2C244.449%2C558.193%2C248.538%5D%2C%5B317.955%2C233.49%2C535.605%2C237.579%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%7D">“Inspired from this point, we propose to use memory network to memorize these extreme events, which is proved to be effective in recognizing inherent patterns contained in historical information [45]”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221117%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1117</span>)</span>

<span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221118%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1118</span>)</span> Predict output o_t = Wo'h_t + b_o with a linear combination of the latent (dimension-reduced) previous observations h_t. Then using those latent observations h_t and the latent representations of each window s_j where t = 1,...,T and j = 1,.., M, predict a probability of whether there was an extreme event at time t. If there was, then we add a linear combination of weights and the product of the probability and event indicator with bias: o_t = \tilde{o}_t + b' \sum p_{tj} q_t.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22H5MYX3M9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221118%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B95.368%2C244.069%2C294.038%2C248.158%5D%2C%5B53.798%2C233.11%2C294.036%2C237.199%5D%2C%5B53.574%2C222.151%2C150.752%2C226.24%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221118%22%7D%7D">“Intuitively, the main advantage of our model lies in, it enables a flexible switch between yielding predictions of normal values and extreme values.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221118%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1118</span>)</span> It's effectively just a two-stage model. Predict the normal output and then predict whether it's likely to be an extreme event and bias the output a bit.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22ELLEH9IS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221118%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B550.988%2C235.97%2C558.19%2C244.936%5D%2C%5B317.955%2C225.011%2C558.197%2C233.977%5D%2C%5B317.955%2C216.276%2C559.71%2C220.365%5D%2C%5B317.955%2C202.172%2C558.361%2C212.06%5D%2C%5B317.417%2C191.213%2C328.186%2C201.101%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221118%22%7D%7D">“in order to influence the distribution of P(Y ), we propose to impose prior of tailed data on loss function. Rather than modeling the output ot directly, we pay our attention to the extreme event indicator ut .”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221118%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1118</span>)</span> It looks like they changed their definition of u_t??

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22ZXAP6EHS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B154.934%2C556.928%2C294.036%2C565.266%5D%2C%5B53.798%2C548.049%2C189.781%2C552.138%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“We call the proposed classification loss function as Extreme Value Loss (EVL).”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span> Entropy based loss function

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%2259IVJKH5%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B265.446%2C491.174%2C294.275%2C499.512%5D%2C%5B53.798%2C482.295%2C295.019%2C486.384%5D%2C%5B53.798%2C468.191%2C294.036%2C480.064%5D%2C%5B53.798%2C460.377%2C131.595%2C464.466%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“The key point of EVL is to find the proper weights by adding approximation, e.g., β0[1 − ut /γ ]γ , on tail distribution of the observations through extreme value theory.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%227YY9UZY9%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B255.869%2C188.169%2C294.041%2C197.136%5D%2C%5B53.798%2C179.435%2C217.912%2C183.524%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“Finally we list the whole parameters to learn as follows.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span> Total Num Pars as a fn of H (the dimension of the latent representations): (6H + 3H^2) + (H + 1) + (2H) + 3 = 3H^2 + 9H + 4  
So for H = 3, we learn 58 parameters. For H = 10, we learn 394 parameters.  
For H = 25, we learn 2104 parameters. So this doesn't scale nicely with increasing dimensionality of the latent reps. If we need 10 obs per param (prob too few), then we will need very long TS to get decent/stable fits.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22ZG8YN6U2%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B327.918%2C361.885%2C558.362%2C365.974%5D%2C%5B317.955%2C350.927%2C558.42%2C355.016%5D%2C%5B317.955%2C339.968%2C558.196%2C344.057%5D%2C%5B317.955%2C329.009%2C506.641%2C333.098%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“For these first two datasets, we set the time length as 500 for training and 200 for testing, while for the last dataset, we randomly extract 150 time series per section with 400 data points, setting the time length as 300 for training and 100 for testing.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span> There's no way these data sets are long enough to estimate the parameters.

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22R78L8SHY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B435.462%2C296.132%2C558.19%2C300.221%5D%2C%5B317.955%2C282.95%2C558.201%2C291.915%5D%2C%5B317.641%2C271.991%2C559.174%2C280.957%5D%2C%5B317.955%2C263.255%2C482.876%2C267.344%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“We further preprocessed the data by replacing the raw data with the difference between time t and t − 1. By a subsequent normalization of data value to a fixed range, the final datasets were therefore constructed.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span>

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22SHJ3EUQ6%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B327.918%2C252.296%2C558.191%2C256.385%5D%2C%5B317.955%2C239.114%2C425.228%2C248.081%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“For each experiment, we conducted a 10-fold cross validation and reported the averaged results.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span> Probably not enough

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22NUCGQ2JZ%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221119%22%2C%22position%22%3A%7B%22pageIndex%22%3A5%2C%22rects%22%3A%5B%5B354.563%2C184.32%2C559.582%2C193.286%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%7D">“Therefore in most cases we chose γ optimally from (2, 3].”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221119%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1119</span>)</span> How?

<span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2FENYDACEU%22%2C%22annotationKey%22%3A%22C2SFKDHE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221120%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B204.745%2C449.565%2C295.414%2C453.654%5D%2C%5B53.798%2C438.606%2C294.035%2C442.695%5D%2C%5B53.798%2C427.648%2C191.938%2C431.737%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221120%22%7D%7D">“The results are in Table 3. Surprisingly, GRU outperformed other baselines although it has the simplest structure in real-world data.”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F4968335%2Fitems%2F2VBD3PYT%22%5D%2C%22locator%22%3A%221120%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Ding et al., 2019, p. 1120</span>)</span>

