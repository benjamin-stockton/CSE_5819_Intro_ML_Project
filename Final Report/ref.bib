@misc{11VectorAutoregressive,
  title = {11.2 {{Vector Autoregressive}} Models {{VAR}}(p) Models | {{STAT}} 510},
  journal = {PennState: Statistics Online Courses},
  abstract = {Enroll today at Penn State World Campus to earn an accredited degree or certificate in Statistics.},
  howpublished = {https://online.stat.psu.edu/stat510/lesson/11/11.2},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\MVJ2AEWL\\11.html}
}

@misc{brownleeMultivariateTimeSeries2017,
  title = {Multivariate {{Time Series Forecasting}} with {{LSTMs}} in {{Keras}}},
  author = {Brownlee, Jason},
  year = {2017},
  month = aug,
  journal = {MachineLearningMastery.com},
  abstract = {Neural networks like Long Short-Term Memory (LSTM) recurrent neural networks are able to almost seamlessly model problems with multiple input variables. This is a great benefit in time series forecasting, where classical linear methods can be difficult to adapt to multivariate or multiple input forecasting problems. In this tutorial, you will discover how you can [\ldots ]},
  langid = {american}
}

@misc{brownleeTimeSeriesForecasting2017,
  title = {Time {{Series Forecasting}} with the {{Long Short-Term Memory Network}} in {{Python}}},
  author = {Brownlee, Jason},
  year = {2017},
  month = apr,
  journal = {MachineLearningMastery.com},
  abstract = {The Long Short-Term Memory recurrent neural network has the promise of learning long sequences of observations. It seems a perfect match for time series forecasting, and in fact, it may be. In this tutorial, you will discover how to develop an LSTM forecast model for a one-step univariate time series forecasting problem. After completing this [\ldots ]},
  langid = {american}
}

@article{cheRecurrentNeuralNetworks2018,
  title = {Recurrent {{Neural Networks}} for {{Multivariate Time Series}} with {{Missing Values}}},
  author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
  year = {2018},
  month = apr,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {6085},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24271-9},
  abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Computational models,Computer science,GRU,Machine learning,Multivariate,Read},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\cheRecurrentNeuralNetworks2018.md;G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Che et al2018\\Che et al_2018_Recurrent Neural Networks for Multivariate Time Series with Missing Values.pdf;C\:\\Users\\stocb\\Zotero\\storage\\EFFEX7WG\\s41598-018-24271-9.html}
}

@misc{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  month = oct,
  number = {arXiv:1409.1259},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.1259},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,GRU,Statistics - Machine Learning,Unread},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Cho et al2014\\Cho et al_2014_On the Properties of Neural Machine Translation.pdf;C\:\\Users\\stocb\\Zotero\\storage\\FGA7GCB9\\1409.html}
}

@misc{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  number = {arXiv:1412.3555},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.3555},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Unread},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Chung et al2014\\Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf;C\:\\Users\\stocb\\Zotero\\storage\\D3LJGR9J\\1412.html}
}

@misc{CompareEffectDifferent,
  title = {Compare the Effect of Different Scalers on Data with Outliers},
  journal = {scikit-learn},
  abstract = {Feature 0 (median income in a block) and feature 5 (average house occupancy) of the California Housing dataset have very different scales and contain some very large outliers. These two characteris...},
  howpublished = {https://scikit-learn/stable/auto\_examples/preprocessing/plot\_all\_scaling.html},
  langid = {english}
}

@misc{DataToolsClimate,
  title = {Data {{Tools}} | {{Climate Data Online}} ({{CDO}}) | {{National Climatic Data Center}} ({{NCDC}})},
  howpublished = {https://www.ncei.noaa.gov/cdo-web/datatools},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\XI5SC8C6\\datatools.html}
}

@inproceedings{dingModelingExtremeEvents2019,
  title = {Modeling {{Extreme Events}} in {{Time Series Prediction}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Ding, Daizong and Zhang, Mi and Pan, Xudong and Yang, Min and He, Xiangnan},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {1114--1122},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330896},
  abstract = {Time series prediction is an intensively studied topic in data mining. In spite of the considerable improvements, recent deep learning-based methods overlook the existence of extreme events, which result in weak performance when applying them to real time series. Extreme events are rare and random, but do play a critical role in many real applications, such as the forecasting of financial crisis and natural disasters. In this paper, we explore the central theme of improving the ability of deep learning on modeling extreme events for time series prediction. Through the lens of formal analysis, we first find that the weakness of deep learning methods roots in the conventional form of quadratic loss. To address this issue, we take inspirations from the Extreme Value Theory, developing a new form of loss called Extreme Value Loss (EVL) for detecting the future occurrence of extreme events. Furthermore, we propose to employ Memory Network in order to memorize extreme events in historical records.By incorporating EVL with an adapted memory network module, we achieve an end-to-end framework for time series prediction with extreme events. Through extensive experiments on synthetic data and two real datasets of stock and climate, we empirically validate the effectiveness of our framework. Besides, we also provide a proper choice for hyper-parameters in our proposed framework by conducting several additional experiments.},
  isbn = {978-1-4503-6201-6},
  keywords = {attention model,extreme event,memory network,Read},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\dingModelingExtremeEvents2019.md;G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Ding et al2019\\Ding et al_2019_Modeling Extreme Events in Time Series Prediction.pdf}
}

@article{friedmanCPSC303ENERGY,
  title = {{{CPSC}} 303: {{ENERGY IN CUBIC SPLINES}}, {{POWER SERIES AS ALGORITHMS}}, {{AND THE INITIAL VALUE PROBLEM}}},
  author = {Friedman, Joel},
  pages = {13},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\MLFWG7BK\\Friedman - CPSC 303 ENERGY IN CUBIC SPLINES, POWER SERIES AS.pdf}
}

@misc{ImputationMissingValues,
  title = {6.4. {{Imputation}} of Missing Values},
  journal = {scikit-learn},
  abstract = {For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which ...},
  howpublished = {https://scikit-learn/stable/modules/impute.html},
  langid = {english}
}

@misc{ImputerSktimeDocumentation,
  title = {Imputer \textemdash{} Sktime Documentation},
  howpublished = {https://www.sktime.org/en/stable/api\_reference/auto\_generated/sktime.transformations.series.impute.Imputer.html}
}

@misc{ImputingMissingValues,
  title = {Imputing Missing Values with Variants of {{IterativeImputer}}},
  journal = {scikit-learn},
  abstract = {The IterativeImputer class is very flexible - it can be used with a variety of estimators to do round-robin regression, treating every variable as an output in turn. In this example we compare some...},
  howpublished = {https://scikit-learn/stable/auto\_examples/impute/plot\_iterative\_imputer\_variants\_comparison.html},
  langid = {english}
}

@misc{IntroAutoencodersTensorFlow,
  title = {Intro to {{Autoencoders}} | {{TensorFlow Core}}},
  journal = {TensorFlow},
  howpublished = {https://www.tensorflow.org/tutorials/generative/autoencoder},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\YGF5IGVW\\autoencoder.html}
}

@misc{kangCustomLossFunction2022,
  title = {Custom {{Loss Function}} in {{Tensorflow}} 2.},
  author = {Kang, Chanseok},
  year = {2022},
  month = feb,
  journal = {Chan`s Jupyter},
  abstract = {In this post, we will learn how to build custom loss functions with function and class. This is the summary of lecture ``Custom Models, Layers and Loss functions with Tensorflow'' from DeepLearning.AI.},
  howpublished = {https://goodboychan.github.io/python/coursera/tensorflow/deeplearning.ai/2022/02/08/01-Tensorflow2-Custom-Loss-Function.html},
  langid = {english}
}

@article{kutzParsimonyUltimateRegularizer2022,
  title = {Parsimony as the Ultimate Regularizer for Physics-Informed Machine Learning},
  author = {Kutz, J. Nathan and Brunton, Steven L.},
  year = {2022},
  month = feb,
  journal = {Nonlinear Dynamics},
  volume = {107},
  number = {3},
  pages = {1801--1817},
  issn = {1573-269X},
  doi = {10.1007/s11071-021-07118-3},
  abstract = {Data-driven modeling continues to be enabled by modern machine learning algorithms and deep learning architectures. The goals of such efforts revolve around the generation of models for prediction, characterization, and control of complex systems. In the context of physics and engineering, extrapolation and generalization are critical aspects of model discovery that are empowered by various aspects of parsimony. Parsimony can be encoded (i) in a low-dimensional coordinate system, (ii) in the representation of governing equations, or (iii) in the representation of parametric dependencies. In what follows, we illustrate techniques that leverage parsimony in deep learning to build physics-based models, culminating in a deep learning architecture that is parsimonious in coordinates and also in representing the dynamics and their parametric dependence through a simple normal form. Ultimately, we argue that promoting parsimony in machine learning results in more physical models, i.e., models that generalize and are parametrically represented by governing equations.},
  langid = {english},
  keywords = {Dynamical systems,Machine learning,Parsimony,Physics,Unread}
}

@article{laptevTimeseriesExtremeEvent,
  title = {Time-Series {{Extreme Event Forecasting}} with {{Neural Networks}} at {{Uber}}},
  author = {Laptev, Nikolay and Yosinski, Jason and Li, Li Erran and Smyl, Slawek},
  pages = {5},
  abstract = {Accurate time-series forecasting during high variance segments (e.g., holidays), is critical for anomaly detection, optimal resource allocation, budget planning and other related tasks. At Uber accurate prediction for completed trips during special events can lead to a more efficient driver allocation resulting in a decreased wait time for the riders.},
  langid = {english},
  keywords = {Unread},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\73N9PL9M\\Laptev et al. - Time-series Extreme Event Forecasting with Neural .pdf}
}

@inproceedings{leaTemporalConvolutionalNetworks2017,
  title = {Temporal {{Convolutional Networks}} for {{Action Segmentation}} and {{Detection}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lea, Colin and Flynn, Michael D. and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
  year = {2017},
  month = jul,
  pages = {1003--1012},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.113},
  abstract = {The ability to identify and temporally segment finegrained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures highlevel temporal patterns. We describe a class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  keywords = {Unread},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\JIXW33DE\\Lea et al. - 2017 - Temporal Convolutional Networks for Action Segment.pdf}
}

@article{liMultistepForecastingOcean2022,
  title = {Multi-Step Forecasting of Ocean Wave Height Using Gate Recurrent Unit Networks with Multivariate Time Series},
  author = {Li, Xinfang and Cao, Jinfeng and Guo, Jihong and Liu, Chao and Wang, Wen and Jia, Zhen and Su, Tianyun},
  year = {2022},
  month = mar,
  journal = {Ocean Engineering},
  volume = {248},
  pages = {110689},
  issn = {0029-8018},
  doi = {10.1016/j.oceaneng.2022.110689},
  abstract = {Ocean wave height is an essential parameter for ocean engineering construction, planning decisions, and coastal hazards assessment. Long-term, accurate, and reliable ocean wave height forecasts are critical for the purposes mentioned above and have attracted more attention in recent years. This work proposed a novel method that achieves robust short-term and long-term ocean wave forecasting via the gate recurrent unit (GRU) network. The GRU-based wave forecasting model is established to learn long-term dependency among multivariate sequential data. The future wave height is predicted based on learned features via the proposed method. Case studies of 6 different stations along the coast of China are investigated. The results show that for 1-hour forecasts, the GRU network is superior to comparison methods in terms of all the error metrics. For 3-hour forecasts, the GRU network shows more robustness compared to the LSTM algorithm. The results also validate that the presented scheme is an efficient and reliable short-term and long-term wave forecasting approach. Applying the forecasting method in reality is essential for ocean safety, ocean exploitation, and many other fields.},
  langid = {english},
  keywords = {Gated recurrent unit network,GRU,Long short-term memory network,Machine learning,multivariate,Multivariate time series,Ocean wave height forecasting,Unread},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Li et al2022\\Li et al_2022_Multi-step forecasting of ocean wave height using gate recurrent unit networks.pdf;C\:\\Users\\stocb\\Zotero\\storage\\62XDG3TF\\S0029801822001469.html}
}

@misc{liptonCriticalReviewRecurrent2015,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  year = {2015},
  month = oct,
  number = {arXiv:1506.00019},
  eprint = {1506.00019},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Read},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\liptonCriticalReviewRecurrent2015.md;C\:\\Users\\stocb\\Zotero\\storage\\SVNC9UMJ\\Lipton et al. - 2015 - A Critical Review of Recurrent Neural Networks for.pdf}
}

@inproceedings{okawaPredictingOpinionDynamics2022,
  title = {Predicting {{Opinion Dynamics}} via {{Sociologically-Informed Neural Networks}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Okawa, Maya and Iwata, Tomoharu},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1306--1316},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539228},
  abstract = {Opinion formation and propagation are crucial phenomena in social networks and have been extensively studied across several disciplines. Traditionally, theoretical models of opinion dynamics have been proposed to describe the interactions between individuals (i.e., social interaction) and their impact on the evolution of collective opinions. Although these models can incorporate sociological and psychological knowledge on the mechanisms of social interaction, they demand extensive calibration with real data to make reliable predictions, requiring much time and effort. Recently, the widespread use of social media platforms provides new paradigms to learn deep learning models from a large volume of social media data. However, these methods ignore any scientific knowledge about the mechanism of social interaction. In this work, we present the first hybrid method called Sociologically-Informed Neural Network (SINN), which integrates theoretical models and social media data by transporting the concepts of physics-informed neural networks (PINNs) from natural science (i.e., physics) into social science (i.e., sociology and social psychology). In particular, we recast theoretical models as ordinary differential equations (ODEs). Then we train a neural network that simultaneously approximates the data and conforms to the ODEs that represent the social scientific knowledge. In addition, we extend PINNs by integrating matrix factorization and a language model to incorporate rich side information (e.g., user profiles) and structural knowledge (e.g., cluster structure of the social interaction network). Moreover, we develop an end-to-end training procedure for SINN, which involves Gumbel-Softmax approximation to include stochastic mechanisms of social interaction. Extensive experiments on real-world and synthetic datasets show SINN outperforms six baseline methods in predicting opinion dynamics.},
  isbn = {978-1-4503-9385-0},
  keywords = {opinion dynamics,Read,sequence prediction,social network},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\okawaPredictingOpinionDynamics2022.md;G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Okawa_Iwata2022\\Okawa_Iwata_2022_Predicting Opinion Dynamics via Sociologically-Informed Neural Networks.pdf}
}

@inproceedings{panUrbanTrafficPrediction2019,
  title = {Urban {{Traffic Prediction}} from {{Spatio-Temporal Data Using Deep Meta Learning}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Pan, Zheyi and Liang, Yuxuan and Wang, Weifeng and Yu, Yong and Zheng, Yu and Zhang, Junbo},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {1720--1730},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330884},
  abstract = {Predicting urban traffic is of great importance to intelligent transportation systems and public safety, yet is very challenging because of two aspects: 1) complex spatio-temporal correlations of urban traffic, including spatial correlations between locations along with temporal correlations among timestamps; 2) diversity of such spatio-temporal correlations, which vary from location to location and depend on the surrounding geographical information, e.g., points of interests and road networks. To tackle these challenges, we proposed a deep-meta-learning based model, entitled ST-MetaNet, to collectively predict traffic in all location at once. ST-MetaNet employs a sequence-to-sequence architecture, consisting of an encoder to learn historical information and a decoder to make predictions step by step. In specific, the encoder and decoder have the same network structure, consisting of a recurrent neural network to encode the traffic, a meta graph attention network to capture diverse spatial correlations, and a meta recurrent neural network to consider diverse temporal correlations. Extensive experiments were conducted based on two real-world datasets to illustrate the effectiveness of ST-MetaNet beyond several state-of-the-art methods.},
  isbn = {978-1-4503-6201-6},
  keywords = {meta learning,neural network,Read,spatio-temporal data,urban traffic},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\panUrbanTrafficPrediction2019.md;G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Pan et al2019\\Pan et al_2019_Urban Traffic Prediction from Spatio-Temporal Data Using Deep Meta Learning.pdf}
}

@misc{PapersCodeTime,
  title = {Papers with {{Code}} - {{Time Series Prediction}}},
  abstract = {The goal of **Time Series Prediction** is to infer the future values of a time series from the past. Source: [Orthogonal Echo State Networks and stochastic evaluations of likelihoods ](https://arxiv.org/abs/1601.05911)},
  howpublished = {https://paperswithcode.com/task/time-series-prediction},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\A3VBMLSE\\time-series-prediction.html}
}

@misc{PapersCodeTimea,
  title = {Papers with {{Code}} - {{Time Series Prediction}}},
  abstract = {The goal of **Time Series Prediction** is to infer the future values of a time series from the past. Source: [Orthogonal Echo State Networks and stochastic evaluations of likelihoods ](https://arxiv.org/abs/1601.05911)},
  howpublished = {https://paperswithcode.com/task/time-series-prediction},
  langid = {english}
}

@misc{Paris2021Weather,
  title = {Paris 2021 {{Past Weather}} ({{France}}) - {{Weather Spark}}},
  howpublished = {https://weatherspark.com/h/y/47913/2021/Historical-Weather-during-2021-in-Paris-France\#Figures-Temperature},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\EZY7VEXA\\Historical-Weather-during-2021-in-Paris-France.html}
}

@misc{rasifaghihiLSTMTensorFlowforTimeseriesforecasting2022,
  title = {{{LSTM-TensorFlow-for-Timeseries-forecasting}}},
  author = {Rasifaghihi, Niousha},
  year = {2022},
  month = sep
}

@misc{rasifaghihiPredictiveAnalyticsTimeSeries2020,
  title = {Predictive {{Analytics}}: {{Time-Series Forecasting}} with {{GRU}} and {{BiLSTM}} in {{TensorFlow}}},
  shorttitle = {Predictive {{Analytics}}},
  author = {Rasifaghihi, Niousha},
  year = {2020},
  month = aug,
  journal = {Medium},
  abstract = {A step-by-step tutorial on building GRU and Bidirectional LSTM for Time-series forecasting},
  howpublished = {https://towardsdatascience.com/predictive-analytics-time-series-forecasting-with-gru-and-bilstm-in-tensorflow-87588c852915},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\6VSCBIEJ\\predictive-analytics-time-series-forecasting-with-gru-and-bilstm-in-tensorflow-87588c852915.html}
}

@misc{remyKerasTCN2022,
  title = {Keras {{TCN}}},
  author = {R{\'e}my, Philippe},
  year = {2022},
  month = sep,
  abstract = {Keras Temporal Convolutional Network.},
  copyright = {MIT},
  keywords = {deep-learning,keras,machine-learning,recurrent-neural-networks,tcn}
}

@article{ribeiroImbalancedRegressionExtreme2020,
  title = {Imbalanced Regression and Extreme Value Prediction},
  author = {Ribeiro, Rita P. and Moniz, Nuno},
  year = {2020},
  month = sep,
  journal = {Machine Learning},
  volume = {109},
  number = {9},
  pages = {1803--1835},
  issn = {1573-0565},
  doi = {10.1007/s10994-020-05900-9},
  abstract = {Research in imbalanced domain learning has almost exclusively focused on solving classification tasks for accurate prediction of cases labelled with a rare class. Approaches for addressing such problems in regression tasks are still scarce due to two main factors. First, standard regression tasks assume each domain value as equally important. Second, standard evaluation metrics focus on assessing the performance of models on the most common values of data distributions. In this paper, we present an approach to tackle imbalanced regression tasks where the objective is to predict extreme (rare) values. We propose an approach to formalise such tasks and to optimise/evaluate predictive models, overcoming the factors mentioned and issues in related work. We present an automatic and non-parametric method to obtain relevance functions, building on the concept of relevance as the mapping of target values into non-uniform domain preferences. Then, we propose SERA, a new evaluation metric capable of assessing the effectiveness and of optimising models towards the prediction of extreme values while penalising severe model bias. An experimental study demonstrates how SERA provides valid and useful insights into the performance of models in imbalanced regression tasks.},
  langid = {english},
  keywords = {Extreme value prediction,Imbalanced domain learning,Imbalanced regression,Supervised learning,Unread},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Ribeiro_Moniz2020\\Ribeiro_Moniz_2020_Imbalanced regression and extreme value prediction.pdf}
}

@misc{RobHyndmanARIMAX2010,
  title = {Rob {{J Hyndman}} - {{The ARIMAX}} Model Muddle},
  year = {2010},
  month = oct,
  howpublished = {https://robjhyndman.com/hyndsight/arimax/},
  langid = {english}
}

@misc{RobHyndmanDetecting2021,
  title = {Rob {{J Hyndman}} - {{Detecting}} Time Series Outliers},
  year = {2021},
  month = aug,
  abstract = {The tsoutliers() function in the forecast package for R is useful for identifying anomalies in a time series. However, it is not properly documented anywhere. This post is intended to fill that gap.},
  howpublished = {https://robjhyndman.com/hyndsight/tsoutliers/},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\NQSUPZ3G\\tsoutliers.html}
}

@article{rouhiardeshiriMultivariateGatedRecurrent2021,
  title = {Multivariate Gated Recurrent Unit for Battery Remaining Useful Life Prediction: {{A}} Deep Learning Approach},
  shorttitle = {Multivariate Gated Recurrent Unit for Battery Remaining Useful Life Prediction},
  author = {Rouhi Ardeshiri, Reza and Ma, Chengbin},
  year = {2021},
  journal = {International Journal of Energy Research},
  volume = {45},
  number = {11},
  pages = {16633--16648},
  issn = {1099-114X},
  doi = {10.1002/er.6910},
  abstract = {This paper proposes the gated recurrent unit (GRU)-recurrent neural network (RNN), a deep learning approach to predict the remaining useful life (RUL) of lithium-ion batteries (LIBs), accurately. The GRU-RNN structure can self-learn the network parameters utilizing adaptive gradient descent algorithms, leading to a reduced computational cost. Unlike the long short-term memory (LSTM) model, GRU-RNN allows time-series dependencies to be tracked between degraded capacities without using any memory cell. This enables the method to predict non-linear capacity degradations and build an explicitly capacity-oriented RUL predictor. Additionally, feature selection based on the random forest technique was used to enhance the prediction precision. The analyses were conducted based on four separate cycling life testing datasets of a lithium-ion battery. The experimental results indicate that the average percentage of root mean square error for the proposed method is about 2\% which respectively is 1.34 times and 8.32 times superior to the LSTM and support vector machine methods. The outcome of this work can be used for managing the Li-ion battery's improvement and optimization.},
  langid = {english},
  keywords = {feature engineering,gated recurrent unit,GRU,lithium-ion battery,multivariate,multivariate time series,remaining useful life,Unread},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/er.6910},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Rouhi Ardeshiri_Ma2021\\Rouhi Ardeshiri_Ma_2021_Multivariate gated recurrent unit for battery remaining useful life prediction.pdf;C\:\\Users\\stocb\\Zotero\\storage\\ZJZPHL8I\\er.html}
}

@article{shivamMultiStepShortTermWind2020,
  title = {Multi-{{Step Short-Term Wind Speed Prediction Using}} a {{Residual Dilated Causal Convolutional Network}} with {{Nonlinear Attention}}},
  author = {Shivam, Kumar and Tzou, Jong-Chyuan and Wu, Shang-Chen},
  year = {2020},
  month = jan,
  journal = {Energies},
  volume = {13},
  number = {7},
  pages = {1772},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1996-1073},
  doi = {10.3390/en13071772},
  abstract = {Wind energy is the most used renewable energy worldwide second only to hydropower. However, the stochastic nature of wind speed makes it harder for wind farms to manage the future power production and maintenance schedules efficiently. Many wind speed prediction models exist that focus on advance neural networks and/or preprocessing techniques to improve the accuracy. Since most of these models require a large amount of historic wind data and are validated using the data split method, the application to real-world scenarios cannot be determined. In this paper, we present a multi-step univariate prediction model for wind speed data inspired by the residual U-net architecture of the convolutional neural network (CNN). We propose a residual dilated causal convolutional neural network (Res-DCCNN) with nonlinear attention for multi-step-ahead wind speed forecasting. Our model can outperform long-term short-term memory networks (LSTM), gated recurrent units (GRU), and Res-DCCNN using sliding window validation techniques for 50-step-ahead wind speed prediction. We tested the performance of the proposed model on six real-world wind speed datasets with different probability distributions to confirm its effectiveness, and using several error metrics, we demonstrated that our proposed model was robust, precise, and applicable to real-world cases.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural network,deep learning architectures,machine learning,residual networks,time series,Unread,wind energy,wind speed forecasting},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Shivam et al2020\\Shivam et al_2020_Multi-Step Short-Term Wind Speed Prediction Using a Residual Dilated Causal.pdf;C\:\\Users\\stocb\\Zotero\\storage\\A8H6FPQW\\1772.html}
}

@book{shumwayTimeSeriesData2019,
  title = {Time {{Series}}: {{A Data Analysis Approach Using R}}},
  author = {Shumway, Robert H. and Stoffer, David S.},
  year = {2019},
  series = {Texts in {{Statistical Science}}},
  edition = {1st},
  publisher = {{CRC Press, Taylor \& Francis Group}},
  address = {{Boca Raton, FL, USA}},
  isbn = {978-0-367-22109-6},
  langid = {english}
}

@misc{singhMultivariateTimeSeries2018,
  title = {Multivariate {{Time Series}} | {{Vector Auto Regression}} ({{VAR}})},
  author = {Singh, Aishwarya},
  year = {2018},
  month = sep,
  journal = {Analytics Vidhya},
  abstract = {Vector Auto Regression method for forecasting multivariate time series uses vectors to represent the relationship between variables and past values.},
  langid = {english}
}

@article{tanDATAGRUDualAttentionTimeAware2020,
  title = {{{DATA-GRU}}: {{Dual-Attention Time-Aware Gated Recurrent Unit}} for {{Irregular Multivariate Time Series}}},
  shorttitle = {{{DATA-GRU}}},
  author = {Tan, Qingxiong and Ye, Mang and Yang, Baoyao and Liu, Siqi and Ma, Andy Jinhua and Yip, Terry Cheuk-Fung and Wong, Grace Lai-Hung and Yuen, PongChi},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {01},
  pages = {930--937},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i01.5440},
  abstract = {Due to the discrepancy of diseases and symptoms, patients usually visit hospitals irregularly and different physiological variables are examined at each visit, producing large amounts of irregular multivariate time series (IMTS) data with missing values and varying intervals. Existing methods process IMTS into regular data so that standard machine learning models can be employed. However, time intervals are usually determined by the status of patients, while missing values are caused by changes in symptoms. Therefore, we propose a novel end-to-end Dual-Attention Time-Aware Gated Recurrent Unit (DATA-GRU) for IMTS to predict the mortality risk of patients. In particular, DATA-GRU is able to: 1) preserve the informative varying intervals by introducing a time-aware structure to directly adjust the influence of the previous status in coordination with the elapsed time, and 2) tackle missing values by proposing a novel dual-attention structure to jointly consider data-quality and medical-knowledge. A novel unreliability-aware attention mechanism is designed to handle the diversity in the reliability of different data, while a new symptom-aware attention mechanism is proposed to extract medical reasons from original clinical records. Extensive experimental results on two real-world datasets demonstrate that DATA-GRU can significantly outperform state-of-the-art methods and provide meaningful clinical interpretation.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {GRU,Multivariate,Read},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\tanDATAGRUDualAttentionTimeAware2020.md;G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819\\Tan et al2020\\Tan et al_2020_DATA-GRU.pdf}
}

@misc{teamKerasDocumentationLSTM,
  title = {Keras Documentation: {{LSTM}} Layer},
  shorttitle = {Keras Documentation},
  author = {Team, Keras},
  abstract = {Keras documentation},
  howpublished = {https://keras.io/api/layers/recurrent\_layers/lstm/},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\FCTJJL6J\\lstm.html}
}

@misc{teamKerasDocumentationTimeseries,
  title = {Keras Documentation: {{Timeseries}} Forecasting for Weather Prediction},
  shorttitle = {Keras Documentation},
  author = {Team, Keras},
  abstract = {Keras documentation},
  howpublished = {https://keras.io/examples/timeseries/timeseries\_weather\_forecasting/},
  langid = {english}
}

@misc{turnerTimeSeriesAnalysis2022,
  title = {Time {{Series Analysis}} and {{Climate Change}}},
  author = {Turner, Peter},
  year = {2022},
  month = sep,
  journal = {Medium},
  abstract = {A hands-on guide to time series analysis; investigating climate data using Python and Facebook's Prophet library},
  howpublished = {https://towardsdatascience.com/time-series-analysis-and-climate-change-7bb4371021e},
  langid = {english}
}

@article{wuDataImputationMultivariate2022,
  title = {Data {{Imputation}} for {{Multivariate Time Series Sensor Data With Large Gaps}} of {{Missing Data}}},
  author = {Wu, Rui and Hamshaw, Scott D. and Yang, Lei and Kincaid, Dustin W. and Etheridge, Randall and Ghasemkhani, Amir},
  year = {2022},
  month = jun,
  journal = {IEEE Sensors Journal},
  volume = {22},
  number = {11},
  pages = {10671--10683},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2022.3166643},
  abstract = {Imputation of missing sensor-collected data is often an important step prior to machine learning and statistical data analysis. One particular data imputation challenge is filling large data gaps when the only related data comes from the same sensor station. In this paper, we propose a framework to improve the popular multivariate imputation by chained equations (MICE) method for dealing with missing data. One key strategy we use to improve model accuracy is to reshape the original sensor data to leverage the correlation between the missing data and the observed data. We demonstrate our framework using data from continuous water quality monitoring stations in Vermont. Because of possible irregularly spaced peaks throughout the time series, the reshaped data is split into extreme and normal values and two MICE models are built. We also recommend that sensor-collected data should be transformed to meet the machine learning model assumptions. According to our experimental results, these strategies can improve MICE data imputation model accuracy at least 23\% for large data gaps based on \$\textbackslash text R\^2\$ values and are promising to be applied for other data imputation algorithms.},
  keywords = {Correlation,Data imputation,Data models,large missing data gap,Machine learning,Mathematical models,Mice,MICE,multivariate,Sensors,time series,Time series analysis,Unread},
  file = {G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819Wu et al2022\\Wu et al_2022_Data Imputation for Multivariate Time Series Sensor Data With Large Gaps of.pdf;C\:\\Users\\stocb\\Zotero\\storage\\YPWDKXAM\\9755143.html}
}

@article{yeTimeseriesModelingPrediction2013,
  title = {Time-Series Modeling and Prediction of Global Monthly Absolute Temperature for Environmental Decision Making},
  author = {Ye, Liming and Yang, Guixia and Van Ranst, Eric and Tang, Huajun},
  year = {2013},
  month = mar,
  journal = {Advances in Atmospheric Sciences},
  volume = {30},
  number = {2},
  pages = {382--396},
  issn = {0256-1530, 1861-9533},
  doi = {10.1007/s00376-012-1252-3},
  abstract = {A generalized, structural, time series modeling framework was developed to analyze the monthly records of absolute surface temperature, one of the most important environmental parameters, using a deterministicstochastic combined (DSC) approach. Although the development of the framework was based on the characterization of the variation patterns of a global dataset, the methodology could be applied to any monthly absolute temperature record. Deterministic processes were used to characterize the variation patterns of the global trend and the cyclic oscillations of the temperature signal, involving polynomial functions and the Fourier method, respectively, while stochastic processes were employed to account for any remaining patterns in the temperature signal, involving seasonal autoregressive integrated moving average (SARIMA) models. A prediction of the monthly global surface temperature during the second decade of the 21st century using the DSC model shows that the global temperature will likely continue to rise at twice the average rate of the past 150 years. The evaluation of prediction accuracy shows that DSC models perform systematically well against selected models of other authors, suggesting that DSC models, when coupled with other ecoenvironmental models, can be used as a supplemental tool for short-term ({$\sim$}10-year) environmental planning and decision making.},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\2UZI6EVE\\Ye et al. - 2013 - Time-series modeling and prediction of global mont.pdf}
}

@inproceedings{zhangDataRegressionFramework2021,
  title = {Data {{Regression Framework}} for {{Time Series Data}} with {{Extreme Events}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Zhang, Yifan and Li, Jiahao and Carlo, Ablan and Manda, Alex K and Hamshaw, Scott and Dascalu, Sergiu M. and Harris, Frederick C. and Wu, Rui},
  year = {2021},
  month = dec,
  pages = {5327--5336},
  doi = {10.1109/BigData52589.2021.9671387},
  abstract = {Time series data are significant to scientific, social, economic, and other areas, such as the prediction of weather changes being instrumental for administrative decision-making. In recent years, deep learning methods have achieved great success in time series prediction when compared with classic machine learning methods. However, because time series data can dynamically change and the correlations between the target variable and other features can also vary, making predictions using time series data is often challenging. To further improve existing machine learning and deep learning models for time series prediction, we propose a framework to integrate machine learning models with anomaly detection algorithms. The extreme events are highlighted so the machine learning models can process them appropriately. We conducted extensive experiments on real-world datasets ranging in size from a few hundred to more than ten thousand records. The experimental results demonstrate that our proposed framework significantly improves machine learning model accuracy and mitigates the accuracy descending rate when the predicting horizon (i.e., the number of timestamps ahead) increases.},
  keywords = {Biological system modeling,Deep learning,Instruments,Machine learning algorithms,Neural Networks,Pipelines,Predictive models,Recurrent Neural Network,Time series analysis,Time Series Analysis,Time Series Prediction,Unread},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\5HGF8X9J\\9671387.html}
}

@article{zhangEnhancingTimeSeries2021,
  title = {Enhancing {{Time Series Predictors}} with {{Generalized Extreme Value Loss}}},
  author = {Zhang, Mi and Ding, Daizong and Pan, Xudong and Yang, Min},
  year = {2021},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3108831},
  abstract = {Time series prediction has wide applications in many safety-critical scenarios. According to previous studies, time series of recorded events usually contain a non-trivial proportion of extreme events, featured with extremely large/small values and may have huge societal consequences if overlooked by a predictive model (i.e., predictor). Despite its significance in time series, we however observe the conventional square loss in time series prediction would ignore the modeling of extreme events. Specifically, we prove the square loss as a learning objective of the predictor behaves equivalently as a Gaussian kernel density estimator (KDE) on the recorded events, which is light-tailed itself and unable to model the ground-truth event distribution, usually heavy-tailed due to the existence of extreme events. Considering the benefits of forecasting extreme events, we propose a unified loss form called Generalized Extreme Value Loss (GEVL), which bridges the misalignment between the tail parts of the estimation and the ground-truth via transformations on either the observed events or the estimator. Following the proposed framework, we present three heavy-tailed kernels and derive the corresponding GEVLs which show different levels of trade-off between modeling effectiveness and computational resources.Comprehensive experiments validate our novel loss form substantially enhances representative time series predictors in modeling extreme events.},
  keywords = {Autoregressive processes,Data models,Extreme Event,Extreme Value Theory,Indexes,Kernel,Memory Network,Predictive models,Read,Task analysis,Time series analysis,Time Series Prediction},
  file = {C\:\\Users\\stocb\\OneDrive - University of Connecticut\\Fall_2022\\CSE_5819_Intro_to_ML\\Project\\Lit_Review\\zhangEnhancingTimeSeries2021.md;G\:\\My Drive\\Zotero\\Course Projects\\CSE 5819Zhang et al2021\\Zhang et al_2021_Enhancing Time Series Predictors with Generalized Extreme Value Loss.pdf;C\:\\Users\\stocb\\Zotero\\storage\\SVV8GKM7\\9527101.html}
}

@misc{RecurrentNeuralNetworks,
  title = {Recurrent {{Neural Networks}} ({{RNN}}) with {{Keras}} | {{TensorFlow Core}}},
  journal = {TensorFlow},
  howpublished = {https://www.tensorflow.org/guide/keras/rnn},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\RBPQMA64\\rnn.html}
}

@misc{TimeSeriesForecasting,
  title = {Time Series Forecasting | {{TensorFlow Core}}},
  journal = {TensorFlow},
  howpublished = {https://www.tensorflow.org/tutorials/structured\_data/time\_series},
  langid = {english},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\LKVDW4M6\\time_series.html}
}


@article{eraslanSinglecellRNAseqDenoising2019,
  title = {Single-Cell {{RNA-seq}} Denoising Using a Deep Count Autoencoder},
  author = {Eraslan, G{\"o}kcen and Simon, Lukas M. and Mircea, Maria and Mueller, Nikola S. and Theis, Fabian J.},
  year = {2019},
  month = dec,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {390},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07931-2},
  langid = {english},
  keywords = {Midterm,Missing},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\S3KN7UQW\\Eraslan et al. - 2019 - Single-cell RNA-seq denoising using a deep count a.pdf}
}

@article{tianModelbasedAutoencodersImputing2020,
  title = {Model-Based Autoencoders for Imputing Discrete Single-Cell {{RNA-seq}} Data},
  author = {Tian, Tian and Min, Martin Renqiang and Wei, Zhi},
  year = {2020},
  month = sep,
  journal = {Methods},
  issn = {1046-2023},
  doi = {10.1016/j.ymeth.2020.09.010},
  abstract = {Deep neural networks have been widely applied for missing data imputation. However, most existing studies have been focused on imputing continuous data, while discrete data imputation is under-explored. Discrete data is common in real world, especially in research areas of bioinformatics, genetics, and biochemistry. In particular, large amounts of recent genomic data are discrete count data generated from single-cell RNA sequencing (scRNA-seq) technology. Most scRNA-seq studies produce a discrete matrix with prevailing `false' zero count observations (missing values). To make downstream analyses more effective, imputation, which recovers the missing values, is often conducted as the first step in pre-processing scRNA-seq data. In this paper, we propose a novel Zero-Inflated Negative Binomial (ZINB) model-based autoencoder for imputing discrete scRNA-seq data. The novelties of our method are twofold. First, in addition to optimizing the ZINB likelihood, we propose to explicitly model the dropout events that cause missing values by using the Gumbel-Softmax distribution. Second, the zero-inflated reconstruction is further optimized with respect to the raw count matrix. Extensive experiments on simulation datasets demonstrate that the zero-inflated reconstruction significantly improves imputation accuracy. Real data experiments show that the proposed imputation can enhance separating different cell types and improve the accuracy of differential expression analysis.},
  langid = {english},
  keywords = {Deep learning,Imputation,scRNA-seq},
  file = {C\:\\Users\\stocb\\Zotero\\storage\\6853TDG4\\Tian et al. - 2020 - Model-based autoencoders for imputing discrete sin.pdf;C\:\\Users\\stocb\\Zotero\\storage\\NWLD9DVQ\\S104620232030205X.html}
}

